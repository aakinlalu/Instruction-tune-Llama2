# Databricks notebook source
# MAGIC %md # Create a model which can create instructions based on input
# MAGIC
# MAGIC ### 1. Define Use cases and Create prompt template 
# MAGIC Convert an idea into a basic prompt template:

# COMMAND ----------

# MAGIC %md
# MAGIC ```
# MAGIC    ### Instruction:       
# MAGIC    Use the Input below to create an instruction, which could have been used to generate the input using an LLM.
# MAGIC ```

# COMMAND ----------

# MAGIC %md ### 2. Create an instruction data 
# MAGIC We will use Dolly, an open-source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.

# COMMAND ----------

# MAGIC %md Install Dependencies 

# COMMAND ----------

# MAGIC %md **Load databricks/databricks-dolly-15k dataset**

# COMMAND ----------

from random import randrange 
from datasets import load_dataset

# Load dataset
dataset = load_dataset("databricks/databricks-dolly-15k", split="train")
print(f"dataset size: {len(dataset)}")

# COMMAND ----------

# MAGIC %md **Define Formatting Function**   
# MAGIC In order to instruct tune model, we will convert structured examples into a collection of tasks described via instruction

# COMMAND ----------

def instruct_fmt(sample):
    return f"""### Instruction:
    Use the Input below to create an instruction, which could have been used to generate the input using an LLM.

    ### Input:
    {sample['response']}

    ### Response:
    {sample['instruction']}
    """

# Test the function
print(instruct_fmt(dataset[randrange(len(dataset))]))
    

# COMMAND ----------

# MAGIC %md ### 3. Instruction-tune Llama 2 using `tri` and the `SFTTrainer`
# MAGIC Quantization Aware Low Rank Adaptar Tuning for Language Generation (QloRA) is a technique used to reduce the memory footprint of LLMs during  finetuning. It does the following:
# MAGIC
# MAGIC * Quamtize the pre-trained model to 4 bits and freeze it.
# MAGIC * Attach small, trainable adapter layers.
# MAGIC * Finetune only the adapter layers while using the frozen quantized model for context.

# COMMAND ----------

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig


use_flash_attention = False 
model_id = "NousResearch/Llama-2-7b-hf"


bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    quantization_config=bnb_config,
    use_cache=False,
    device_map="auto"
    )

model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# COMMAND ----------

from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model

# LoRA config based on QLoRA paper
peft_config = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        bias="none",
        task_type="CAUSAL_LM",
)


# prepare model for training
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)

# COMMAND ----------

# MAGIC %md **Define the hyperparameters**

# COMMAND ----------

from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="llama-7-int4-dolly",
    num_train_epochs=3,
    per_device_train_batch_size=6 if use_flash_attention else 4,
    gradient_accumulation_steps=2,
    gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    bf16=True,
    tf32=True,
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="constant",
    disable_tqdm=True 
)


# COMMAND ----------

# MAGIC %md **Create SFTTrainer to start training the model**

# COMMAND ----------

from trl import SFTTrainer

max_seq_length = 2048 # max sequence length for model and packing of the dataset

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    packing=True,
    formatting_func=format_instruction,
    args=args,
)


# COMMAND ----------

# MAGIC %md **Start training the model**

# COMMAND ----------

# train
trainer.train() # there will not be a progress bar since tqdm is disabled

# save model
trainer.save_model()


# COMMAND ----------

# MAGIC %md ### 4. Test Model and run Inference 

# COMMAND ----------

import torch
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer

args.output_dir = "llama-7-int4-dolly"

# load base LLM model and tokenizer
model = AutoPeftModelForCausalLM.from_pretrained(
    args.output_dir,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16,
    load_in_4bit=True,
)
tokenizer = AutoTokenizer.from_pretrained(args.output_dir)

# COMMAND ----------

# MAGIC %md **Load the dataset again with a random sample to try to generate an instruction**

# COMMAND ----------

from datasets import load_dataset
from random import randrange


# Load dataset from the hub and get a sample
dataset = load_dataset("databricks/databricks-dolly-15k", split="train")
sample = dataset[randrange(len(dataset))]

prompt = f"""### Instruction:
Use the Input below to create an instruction, which could have been used to generate the input using an LLM.

### Input:
{sample['response']}

### Response:
"""

input_ids = tokenizer(prompt, return_tensors="pt", truncation=True).input_ids.cuda()
# with torch.inference_mode():
outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)

print(f"Prompt:\n{sample['response']}\n")
print(f"Generated instruction:\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}")
print(f"Ground truth:\n{sample['instruction']}")


# COMMAND ----------

from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    args.output_dir,
    low_cpu_mem_usage=True,
)

# Merge LoRA and base model
merged_model = model.merge_and_unload()

# Save the merged model
merged_model.save_pretrained("merged_model",safe_serialization=True)
tokenizer.save_pretrained("merged_model")


# COMMAND ----------

# MAGIC %md ### References:
# MAGIC ![Extended Guide: Instruction-tune Llama 2](https://www.philschmid.de/instruction-tune-llama-2)
